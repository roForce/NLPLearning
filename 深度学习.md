#常见的优化算法
#梯度下降
#随机梯度下降：随机的从样本中抽出一个样本进行梯度的更新
#小批量梯度下降：找一波数据计算密度，使用均值更新参数
#动量法：Momentum优化器，基于梯度的移动指数加权平均，对网络的梯度进行平滑处理，让梯度的摆动幅度变得更加小
#AdaGrad
    #就是将每一个参数的每一次迭代的梯度平方后累加再开平方，用全局学习率除以这个数，作为学习率的更新，从而达到自适应学习率的效果
    
#RMSProp：

#数据加载
1.数据集类
我们需要在自定义的数据集类中继承DataSet[i]中获取其中的第i条数据
需要实现两个方法
    1.__len__方法，能够实现通过全局的len()方法获取其中的元素的个数
    2.__getitem__方法，能够通过引入索引的方式获取数据，例如通过dataSet[i]获取其中的第i个数据
实现如下：
```cython
import torch
from torch.utils.data import Dataset
from torch.utils.data import  DataLoader
data_path = r"./SMSSpamCollection"
class MyDataSet(Dataset):
    def __init__(self):
        self.lines = open(data_path).readlines()
        print("运行了")
    def __getitem__(self, index):
        #获取索引对应位置的一条数据
        #分开标签
        cur_line = self.lines[index].strip()
        label = cur_line[:4].strip()
        content = cur_line[4:].strip()
        #fan
        return label,content
    
    def __len__(self):
        #返回数据的总数量
        return len(self.lines)


my_dataset = MyDataSet()
print(my_dataset[0])
print(my_dataset.lines)
print(len(my_dataset))
    

```
### 2. 迭代数据集
    1. 批处理数据
    2. 打乱数据
    3. 使用多线程multiprocessing并行加载数据
```cython
import torch
from torch.utils.data import Dataset
from torch.utils.data import  DataLoader
data_path = r"./SMSSpamCollection"
class MyDataSet(Dataset):
    def __init__(self):
        self.lines = open(data_path).readlines()
        print("运行了")
    def __getitem__(self, index):
        #获取索引对应位置的一条数据
        #分开标签
        cur_line = self.lines[index].strip()
        label = cur_line[:4].strip()
        content = cur_line[4:].strip()
        #fan
        return label,content
    
    def __len__(self):
        #返回数据的总数量
        return len(self.lines)

if __name__ == 'main':
    my_dataset = MyDataSet()
    #shuffle为True的时候表示是打乱的
    #当drop_last为true的时候会把最后一个batch给删除
    data_loader = DataLoader(dataset=my_dataset,batch_size=2,shuffle=True,drop_last=True)
    for i in data_loader:
        print(i)
        break
    # len(dataset) = 数据集的样本数
    # len(dataloader) = math.ceil(样本数/batchsize) 也就是说向上取整
    #enumerate可以遍历参数索引
    for index,(label,context) in enumerate(data_loader):
        print(index,label,context)

```
### 3.pytorch中自带的数据集
    1.由上层的两个api提供，分别是torchvision,torchtext
```cython
from torchvision.datasets import MNIST
#MINST是手写数据的识别，第一次使用的时候需要将Download参数设置为True，之后设置为False
minist = MNIST(root="./data",train=True,download=False)
print(minist[0][0].show())
```
### 4.torchvision.transforms图形数据处理方法
    1.把一个取值范围为[0,255]的PIL.Image或者shape为（H，W，C）的numpy.ndarray，转换成形状为[C,H,W],取值范围是[0,1.0]的torch.FloatTensor
        其中（H,W,C）指的是（高，宽，通道数）
    2. 使用toTensor的api
```cython
from torchvision.datasets import MNIST
from torchvision import transforms
#MINST是手写数据的识别，第一次使用的时候需要将Download参数设置为True，之后设置为False
minist = MNIST(root="./data",train=True,download=False)
ret = transforms.ToTensor()(minist[0][0])
#可以看出将通道数，高，宽交换过来了
print(ret.size())
```
    3. Normalize(mean,std)

```cython
from torchvision.datasets import MNIST
from torchvision import transforms
#MINST是手写数据的识别，第一次使用的时候需要将Download参数设置为True，之后设置为False
minist = MNIST(root="./data",train=True,download=False)
ret = transforms.ToTensor()(minist[0][0])
#可以看出将通道数，高，宽交换过来了
print(ret.size())
#前面一个元组与后面一个标准差的维度必须图片维度相同
#其实就是(image - mean)/std
norm_img = transforms.Normalize((10,10,10)(1,1,1))(img)
```
    4.Compose()
        数据经过list中的每一个方法挨个进行处理

```cython
import torchvision.transforms
from torchvision.datasets import MNIST
from torchvision import transforms

transforms.Compose
(
    [torchvision.transforms.ToTensor()#先转换成tensor
    torchvision.transforms.Normalize(mean(),std())#再进行正则化
     ]

)
```
### 4.模型的构建
    a.激活函数的使用
        import torch.nn.functional as F
        F.relu(b)
    b.每一层数据的形状
        原始输入的数据的形状
        进行形状的修改
        定义第一层输入和输出
        激活函数进行激活
        
    c.模型的损失函数
    交叉熵损失：我们把softmax概率传入对数似然损失得到的损失函数称为交叉熵损失
    普遍运用的方法实现交叉熵损失的方法如下
```cython
#对输出的值计算机softmax取对数
output = F.log_softmax(x,dim=-1)
#使用torch中的带权损失
loss = F.nll_loss(output,target)

```
    带权损失定义为 l_n =-/sum(w_i * x _i) 就是将log(P)作为x_i,把真实值Y作为权重

### 模型的保存
    1.保存模型参数
        torch.save(minst_net.state_dict(),"model/mnist_net.pt")
    2.保存优化器参数
        torch.save(optimizer.state-dict(),'reaults/minst_optimizer.pt')
### 模型的加载
    mnist_net.load_state_dict(torch.load("model/mnist_net.pt")
    optimizer.load_state_dict(torch.load('reaults/minst_optimizer.pt'))

### 模型的评估
    



        
                


